{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Trabalho Final - Processamento Digital de Imagens**"
      ],
      "metadata": {
        "id": "Y5tdro5dN8dO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*EfficientEnsemble: Diagnóstico de câncer de mama em imagens de ultrassom utilizando processamento de imagens e Ensemble de EfficientNets*"
      ],
      "metadata": {
        "id": "oSZAPX1QOUKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificação do Ambiente e Versões"
      ],
      "metadata": {
        "id": "Nnarr5ZY_Zpl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Emj4XtCace3s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf, sys, numpy as np, os, platform\n",
        "print(\"TF:\", tf.__version__)\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"OS:\", platform.platform())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extrair pasta .zip"
      ],
      "metadata": {
        "id": "vklbB7le_fut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/archive.zip\" -d /content/"
      ],
      "metadata": {
        "id": "rCB_hPXP7H58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependências para o filtro guiado"
      ],
      "metadata": {
        "id": "tvf8CIqAR4H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install opencv-contrib-python-headless==4.10.0.84"
      ],
      "metadata": {
        "id": "30VyRJqeR0FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports, Configurações Gerais e Reprodutibilidade\n"
      ],
      "metadata": {
        "id": "zjNk-IbF_o9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, random, math, pathlib, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 18\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "IMG_SIZE_B0 = (224, 224)\n",
        "IMG_SIZE_B1 = (240, 240)\n",
        "IMG_SIZE_B2 = (260, 260)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "LR = 1e-4\n",
        "\n",
        "BASE_DIR = \"/content/Dataset_BUSI_with_GT\""
      ],
      "metadata": {
        "id": "ZDSOpTQe7Gb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coleta das Imagens por Classe e Construção do DataFrame Unificado (excluindo máscaras)"
      ],
      "metadata": {
        "id": "VVaZ3eEx_sBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import glob\n",
        "\n",
        "def list_images_no_masks(class_dir, label):\n",
        "    paths = []\n",
        "    for root, _, files in os.walk(class_dir):\n",
        "        for f in files:\n",
        "            lf = f.lower()\n",
        "            if lf.endswith((\".png\", \".jpg\", \".jpeg\")) and (\"mask\" not in lf):\n",
        "                paths.append(os.path.join(root, f))\n",
        "    labels = [label] * len(paths)\n",
        "    return paths, labels\n",
        "\n",
        "benign_dir    = os.path.join(BASE_DIR, \"benign\")\n",
        "malignant_dir = os.path.join(BASE_DIR, \"malignant\")\n",
        "\n",
        "assert os.path.isdir(benign_dir) and os.path.isdir(malignant_dir), \\\n",
        "    \"Pastas 'benign' e/ou 'malignant' não encontradas.\"\n",
        "\n",
        "benign_paths, benign_labels       = list_images_no_masks(benign_dir, 0)\n",
        "malignant_paths, malignant_labels = list_images_no_masks(malignant_dir, 1)\n",
        "\n",
        "print(\"CONTAGEM DE IMAGENS (SEM MÁSCARAS): \")\n",
        "print(f\"Imagens benignas  : {len(benign_paths)}\")\n",
        "print(f\"Imagens malignas  : {len(malignant_paths)}\")\n",
        "print(f\"Total de imagens  : {len(benign_paths) + len(malignant_paths)}\\n\")\n",
        "\n",
        "all_paths  = benign_paths + malignant_paths\n",
        "all_labels = benign_labels + malignant_labels\n",
        "\n",
        "df = pd.DataFrame({\"path\": all_paths, \"label\": all_labels})\n",
        "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "print(\"Value counts em df (1 linha por imagem):\")\n",
        "print(df[\"label\"].value_counts().rename({0: \"benign\", 1: \"malignant\"}))\n",
        "print(\"\\nPrimeiras imagens:\")\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "id": "Ptb2J8OQ7sx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divisão Estratificada do Dataset em Treino, Validação e Teste (60/20/20)\n"
      ],
      "metadata": {
        "id": "CY_cw-59_xu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.4, stratify=df[\"label\"], random_state=SEED\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=SEED\n",
        ")\n",
        "\n",
        "def describe_split(name, dframe):\n",
        "    counts = dframe[\"label\"].value_counts().rename({0:\"benign\",1:\"malignant\"})\n",
        "    print(f\"{name}: {len(dframe)} amostras |\", dict(counts))\n",
        "\n",
        "describe_split(\"Treino\", train_df)\n",
        "describe_split(\"Val\", val_df)\n",
        "describe_split(\"Teste\", test_df)"
      ],
      "metadata": {
        "id": "GUD9rKwd8AY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extração da ROI a partir das máscaras (sem pré-processamento)\n",
        "\n"
      ],
      "metadata": {
        "id": "UAzeI3lB_-x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "ROI_DIR = \"/content/ROI_BUSI\"\n",
        "os.makedirs(ROI_DIR, exist_ok=True)\n",
        "\n",
        "def bbox_from_mask(mask: np.ndarray):\n",
        "    \"\"\"Retorna bounding box da maior componente conexa da máscara.\"\"\"\n",
        "    m = (mask > 0).astype(np.uint8)\n",
        "    num, _, stats, _ = cv2.connectedComponentsWithStats(m, connectivity=8)\n",
        "\n",
        "    if num <= 1:\n",
        "        ys, xs = np.where(m)\n",
        "        if ys.size == 0:\n",
        "            return (0, 0, mask.shape[1], mask.shape[0])\n",
        "        x0, y0, x1, y1 = xs.min(), ys.min(), xs.max(), ys.max()\n",
        "        return (x0, y0, x1 - x0 + 1, y1 - y0 + 1)\n",
        "\n",
        "    idx = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
        "    x, y, w, h, _ = stats[idx]\n",
        "    return (x, y, w, h)\n",
        "\n",
        "\n",
        "def select_largest_mask_path(img_path: str):\n",
        "    \"\"\"\n",
        "    Dada uma imagem, retorna o caminho da máscara com maior área (>0).\n",
        "    Se não encontrar máscara válida, retorna None.\n",
        "    \"\"\"\n",
        "    p = Path(img_path)\n",
        "    mask_pattern = p.with_name(f\"{p.stem}_mask*.png\")\n",
        "    mask_paths = sorted(glob.glob(str(mask_pattern)))\n",
        "\n",
        "    best_path = None\n",
        "    best_area = 0\n",
        "\n",
        "    for mpath in mask_paths:\n",
        "        mask = cv2.imread(mpath, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            print(f\"[WARNING] não foi possível ler a máscara: {mpath}\")\n",
        "            continue\n",
        "\n",
        "        area = np.count_nonzero(mask)\n",
        "        if area > best_area:\n",
        "            best_area = area\n",
        "            best_path = mpath\n",
        "\n",
        "    return best_path\n",
        "\n",
        "\n",
        "def extract_roi_largest_mask(img_path: str, out_root: str):\n",
        "    \"\"\"\n",
        "    Extrai uma ROI por imagem, usando apenas a máscara de maior área.\n",
        "    Se não houver máscara legível, usa a imagem inteira como ROI.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "    if img is None:\n",
        "        print(f\"[WARNING] imagem não encontrada: {img_path}\")\n",
        "        return None\n",
        "\n",
        "    best_mask_path = select_largest_mask_path(img_path)\n",
        "\n",
        "    if best_mask_path is not None:\n",
        "        mask = cv2.imread(best_mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is not None:\n",
        "            x, y, w, h = bbox_from_mask(mask)\n",
        "        else:\n",
        "            print(f\"[WARNING] falha ao ler máscara {best_mask_path}, usando imagem inteira.\")\n",
        "            h_img, w_img = img.shape[:2]\n",
        "            x, y, w, h = 0, 0, w_img, h_img\n",
        "    else:\n",
        "        h_img, w_img = img.shape[:2]\n",
        "        x, y, w, h = 0, 0, w_img, h_img\n",
        "\n",
        "    crop = img[y:y+h, x:x+w]\n",
        "\n",
        "    p = Path(img_path)\n",
        "    cls = p.parent.name\n",
        "    out_dir = Path(out_root) / cls\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    out_name = f\"{p.stem}_roi.png\"\n",
        "    out_path = out_dir / out_name\n",
        "    cv2.imwrite(str(out_path), crop)\n",
        "\n",
        "    return str(out_path)\n",
        "\n",
        "roi_paths = []\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extraindo ROIs (1 por imagem)\"):\n",
        "    roi_path = extract_roi_largest_mask(row[\"path\"], ROI_DIR)\n",
        "    if roi_path is None:\n",
        "        print(f\"[WARNING] Não foi possível gerar ROI para {row['path']}\")\n",
        "        roi_path = row[\"path\"]\n",
        "    roi_paths.append(roi_path)\n",
        "\n",
        "roi_df = pd.DataFrame({\n",
        "    \"path\": roi_paths,\n",
        "    \"label\": df[\"label\"].values\n",
        "})\n",
        "\n",
        "print(\"ROIs salvas:\", len(roi_df))\n",
        "print(roi_df[\"label\"].value_counts().rename({0: \"benign\", 1: \"malignant\"}))\n",
        "roi_df.head()"
      ],
      "metadata": {
        "id": "XK3vDpzMOX20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-processamento da ROI (Filtro Guiado e Mediana)"
      ],
      "metadata": {
        "id": "tjPycd48g6qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "ROI_PP_DIR = \"/content/ROI_PP_BUSI\"\n",
        "os.makedirs(ROI_PP_DIR, exist_ok=True)\n",
        "\n",
        "def guided_then_median(bgr_img: np.ndarray, radius=8, eps=1e-2):\n",
        "    try:\n",
        "        gf = cv2.ximgproc.guidedFilter\n",
        "        guided = gf(guide=bgr_img, src=bgr_img, radius=radius, eps=eps)\n",
        "    except Exception:\n",
        "        guided = cv2.bilateralFilter(bgr_img, d=9, sigmaColor=75, sigmaSpace=75)\n",
        "    median = cv2.medianBlur(guided, ksize=3)\n",
        "    return median\n",
        "\n",
        "def apply_pp_to_roi(roi_path: str, out_root: str):\n",
        "    img = cv2.imread(roi_path, cv2.IMREAD_COLOR)\n",
        "    assert img is not None, f\"Não foi possível ler a ROI: {roi_path}\"\n",
        "    img_pp = guided_then_median(img)\n",
        "\n",
        "    p = Path(roi_path); cls = p.parent.name\n",
        "    out_dir = Path(out_root) / cls\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out_path = out_dir / p.name\n",
        "    if not out_path.exists():\n",
        "        cv2.imwrite(str(out_path), img_pp)\n",
        "    return str(out_path)\n",
        "\n",
        "pp_paths, pp_labels = [], []\n",
        "for idx, row in roi_df.iterrows():\n",
        "    pp_path = apply_pp_to_roi(row[\"path\"], ROI_PP_DIR)\n",
        "    pp_paths.append(pp_path); pp_labels.append(row[\"label\"])\n",
        "\n",
        "pp_df = pd.DataFrame({\"path\": pp_paths, \"label\": pp_labels})\n",
        "train_pp_df = pp_df.loc[train_df.index].reset_index(drop=True)\n",
        "val_pp_df   = pp_df.loc[val_df.index].reset_index(drop=True)\n",
        "test_pp_df  = pp_df.loc[test_df.index].reset_index(drop=True)\n",
        "\n",
        "print(\"ROI+PP salvas:\", len(pp_df))"
      ],
      "metadata": {
        "id": "_cYeaNwK4n9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aumento apenas na minoria (malignant) até igualar"
      ],
      "metadata": {
        "id": "fzGZDV6C4pGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "AUG_DIR = \"/content/ROI_PP_AUG_BUSI\"\n",
        "os.makedirs(AUG_DIR, exist_ok=True)\n",
        "(AUG_DIR_PATH := Path(AUG_DIR)).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "train_benign = train_pp_df[train_pp_df[\"label\"] == 0].reset_index(drop=True)\n",
        "train_malig  = train_pp_df[train_pp_df[\"label\"] == 1].reset_index(drop=True)\n",
        "\n",
        "target_malig = len(train_benign)\n",
        "need = target_malig - len(train_malig)\n",
        "print(f\"Benignas: {len(train_benign)} | Malignas: {len(train_malig)} | Precisamos criar: {need}\")\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "P_FLIP_H = 0.5\n",
        "P_FLIP_V = 0.5\n",
        "P_ROTATE = 0.5\n",
        "P_ZOOM   = 0.5\n",
        "\n",
        "\n",
        "MAX_DEG   = 5\n",
        "MAX_ZOOM  = 0.10\n",
        "\n",
        "\n",
        "def random_small_rotate(img, max_deg=MAX_DEG):\n",
        "    \"\"\"Roda a imagem por um ângulo aleatório entre -max_deg e +max_deg.\n",
        "       Usa borderMode reflect para evitar bordas pretas.\"\"\"\n",
        "    deg = rng.uniform(-max_deg, max_deg)\n",
        "    h, w = img.shape[:2]\n",
        "    M = cv2.getRotationMatrix2D((w/2, h/2), deg, 1.0)\n",
        "    return cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
        "\n",
        "def random_small_zoom(img, max_ratio=MAX_ZOOM):\n",
        "    \"\"\"Zoom in/out até ±max_ratio e recorta/letterbox para manter tamanho original.\n",
        "       Se z >= 1.0 -> recorta o centro (zoom in).\n",
        "       Se z <  1.0 -> faz letterbox com reflexo (evita bordas pretas).\"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    z = 1.0 + rng.uniform(-max_ratio, max_ratio)\n",
        "    nh, nw = int(round(h*z)), int(round(w*z))\n",
        "    resized = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "    if z >= 1.0:\n",
        "        y0 = (nh - h)//2; x0 = (nw - w)//2\n",
        "        return resized[y0:y0+h, x0:x0+w]\n",
        "    else:\n",
        "        top = (h-nh)//2\n",
        "        bottom = h-nh-top\n",
        "        left = (w-nw)//2\n",
        "        right = w-nw-left\n",
        "        out = cv2.copyMakeBorder(resized, top, bottom, left, right, borderType=cv2.BORDER_REFLECT_101)\n",
        "        return out\n",
        "\n",
        "def apply_random_aug(img):\n",
        "    \"\"\"\n",
        "    Aplica cada transformação condicionalmente/independentemente.\n",
        "    \"\"\"\n",
        "    ops = [\"flip_h\", \"flip_v\", \"rotate\", \"zoom\"]\n",
        "    rng.shuffle(ops)\n",
        "\n",
        "    for op in ops:\n",
        "        r = rng.random()\n",
        "        if op == \"flip_h\" and r < P_FLIP_H:\n",
        "            img = cv2.flip(img, 1)\n",
        "        elif op == \"flip_v\" and r < P_FLIP_V:\n",
        "            img = cv2.flip(img, 0)\n",
        "        elif op == \"rotate\" and r < P_ROTATE:\n",
        "            img = random_small_rotate(img)\n",
        "        elif op == \"zoom\" and r < P_ZOOM:\n",
        "            img = random_small_zoom(img)\n",
        "\n",
        "\n",
        "    return img\n",
        "\n",
        "aug_rows = []\n",
        "if need > 0:\n",
        "    src_paths = train_malig[\"path\"].tolist()\n",
        "    i = 0\n",
        "    max_attempts = need * 10\n",
        "    attempts = 0\n",
        "    while len(aug_rows) < need and attempts < max_attempts:\n",
        "        p = Path(src_paths[i % len(src_paths)])\n",
        "        img = cv2.imread(str(p), cv2.IMREAD_COLOR)\n",
        "        assert img is not None, f\"Falha ao ler {p}\"\n",
        "\n",
        "        img_aug = apply_random_aug(img)\n",
        "\n",
        "        out_dir = AUG_DIR_PATH / \"malignant\"\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        out_name = f\"{p.stem}_aug_{len(aug_rows):05d}{p.suffix}\"\n",
        "        out_path = out_dir / out_name\n",
        "        cv2.imwrite(str(out_path), img_aug)\n",
        "\n",
        "        aug_rows.append({\"path\": str(out_path), \"label\": 1})\n",
        "        i += 1\n",
        "        attempts += 1\n",
        "\n",
        "    if len(aug_rows) != need:\n",
        "        raise RuntimeError(f\"Foi gerado {len(aug_rows)} augmentações, mas eram esperadas {need}.\")\n",
        "\n",
        "aug_df = pd.DataFrame(aug_rows)\n",
        "train_pp_equal_df = pd.concat([train_benign, train_malig, aug_df], ignore_index=True)\n",
        "train_pp_equal_df = train_pp_equal_df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "print(\"Novo treino balanceado por AD (em disco):\", train_pp_equal_df[\"label\"].value_counts().to_dict())"
      ],
      "metadata": {
        "id": "LJT2IkJAJLFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decodificação/Resize e construção dos datasets (com treino balanceado)"
      ],
      "metadata": {
        "id": "VtghukW71Qx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_img(path, label, resize_to):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, resize_to, method=\"bilinear\")\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    return img, tf.cast(label, tf.int32)\n",
        "\n",
        "def make_dataset(df_, resize_to=(224,224), training=False, shuffle=True):\n",
        "    paths = df_[\"path\"].values\n",
        "    labels = df_[\"label\"].values\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    ds = ds.map(lambda p, y: decode_img(p, y, resize_to), num_parallel_calls=AUTOTUNE)\n",
        "    if training and shuffle:\n",
        "        ds = ds.shuffle(4096, seed=SEED, reshuffle_each_iteration=True)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "val_ds_b0   = make_dataset(val_pp_df,   resize_to=IMG_SIZE_B0, training=False, shuffle=False)\n",
        "test_ds_b0  = make_dataset(test_pp_df,  resize_to=IMG_SIZE_B0, training=False, shuffle=False)\n",
        "val_ds_b1   = make_dataset(val_pp_df,   resize_to=IMG_SIZE_B1, training=False, shuffle=False)\n",
        "test_ds_b1  = make_dataset(test_pp_df,  resize_to=IMG_SIZE_B1, training=False, shuffle=False)\n",
        "val_ds_b2   = make_dataset(val_pp_df,   resize_to=IMG_SIZE_B2, training=False, shuffle=False)\n",
        "test_ds_b2  = make_dataset(test_pp_df,  resize_to=IMG_SIZE_B2, training=False, shuffle=False)\n",
        "\n",
        "train_ds_b0 = make_dataset(train_pp_equal_df, resize_to=IMG_SIZE_B0, training=True, shuffle=True)\n",
        "train_ds_b1 = make_dataset(train_pp_equal_df, resize_to=IMG_SIZE_B1, training=True, shuffle=True)\n",
        "train_ds_b2 = make_dataset(train_pp_equal_df, resize_to=IMG_SIZE_B2, training=True, shuffle=True)"
      ],
      "metadata": {
        "id": "rv8gJYqD45gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos EfficientNet (B0/B1/B2) com camadas de Aumento de Dados"
      ],
      "metadata": {
        "id": "MrQhEF3s1hYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_effnet(model_name: str, img_size):\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "    x = layers.Lambda(keras.applications.efficientnet.preprocess_input, name=\"effnet_preprocess\")(inputs)\n",
        "\n",
        "    if model_name == \"B0\":\n",
        "        base = EfficientNetB0(include_top=False, input_shape=img_size + (3,), weights=\"imagenet\")\n",
        "    elif model_name == \"B1\":\n",
        "        base = EfficientNetB1(include_top=False, input_shape=img_size + (3,), weights=\"imagenet\")\n",
        "    elif model_name == \"B2\":\n",
        "        base = EfficientNetB2(include_top=False, input_shape=img_size + (3,), weights=\"imagenet\")\n",
        "    else:\n",
        "        raise ValueError(\"model_name deve ser B0, B1 ou B2.\")\n",
        "\n",
        "    base.trainable = True\n",
        "    x = base(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs, name=f\"EfficientNet{model_name}\")\n",
        "\n",
        "    opt = keras.optimizers.Adam(learning_rate=LR)\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            \"accuracy\",\n",
        "            keras.metrics.Precision(name=\"precision\"),\n",
        "            keras.metrics.Recall(name=\"recall\"),\n",
        "            keras.metrics.AUC(name=\"auc\"),\n",
        "        ],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "ckpts = {\n",
        "    \"B0\": \"/content/effb0_roi_pp_ad.weights.h5\",\n",
        "    \"B1\": \"/content/effb1_roi_pp_ad.weights.h5\",\n",
        "    \"B2\": \"/content/effb2_roi_pp_ad.weights.h5\",\n",
        "}"
      ],
      "metadata": {
        "id": "ugBTYA-l4-V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ],
      "metadata": {
        "id": "O0oyPFbl1tKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histories = {}\n",
        "\n",
        "def train_one(model_name, img_size, train_ds, val_ds):\n",
        "    model = build_effnet(model_name, img_size)\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            ckpts[model_name], monitor=\"val_loss\",\n",
        "            save_best_only=True, save_weights_only=True, verbose=1\n",
        "        ),\n",
        "\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=EPOCHS,\n",
        "        verbose=1,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    histories[model_name] = history.history\n",
        "    return model\n",
        "\n",
        "model_b0 = train_one(\"B0\", IMG_SIZE_B0, train_ds_b0, val_ds_b0)\n",
        "model_b1 = train_one(\"B1\", IMG_SIZE_B1, train_ds_b1, val_ds_b1)\n",
        "model_b2 = train_one(\"B2\", IMG_SIZE_B2, train_ds_b2, val_ds_b2)"
      ],
      "metadata": {
        "id": "dN-ptxw35Dkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados"
      ],
      "metadata": {
        "id": "6hRdqJRk2Lz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_ds, name=\"\"):\n",
        "    y_prob = model.predict(test_ds, verbose=0).ravel()\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "    y_true = np.concatenate([y.numpy() for _, y in test_ds.unbatch().batch(1024)], axis=0)\n",
        "\n",
        "    acc  = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec  = recall_score(y_true, y_pred)\n",
        "    f1   = f1_score(y_true, y_pred)\n",
        "    auc  = roc_auc_score(y_true, y_prob)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "    print(f\"\\nResultados — TESTE ({name})\")\n",
        "    print(f\"Acurácia:       {acc*100:6.2f}%\")\n",
        "    print(f\"Especificidade: {specificity*100:6.2f}%\")\n",
        "    print(f\"Sensibilidade:  {rec*100:6.2f}%\")\n",
        "    print(f\"Precisão:       {prec*100:6.2f}%\")\n",
        "    print(f\"F1-score:       {f1*100:6.2f}%\")\n",
        "    print(f\"AUC-ROC:        {auc:8.5f}\")\n",
        "    return y_true, y_prob, y_pred\n",
        "\n",
        "\n",
        "model_b0.load_weights(ckpts[\"B0\"])\n",
        "model_b1.load_weights(ckpts[\"B1\"])\n",
        "model_b2.load_weights(ckpts[\"B2\"])\n",
        "\n",
        "# Avaliação individual\n",
        "y_true_b0, y_prob_b0, y_pred_b0 = evaluate_model(model_b0, test_ds_b0, name=\"ROI+PP+AD + ENB0\")\n",
        "y_true_b1, y_prob_b1, y_pred_b1 = evaluate_model(model_b1, test_ds_b1, name=\"ROI+PP+AD + ENB1\")\n",
        "y_true_b2, y_prob_b2, y_pred_b2 = evaluate_model(model_b2, test_ds_b2, name=\"ROI+PP+AD + ENB2\")\n",
        "\n",
        "assert np.array_equal(y_true_b0, y_true_b1) and np.array_equal(y_true_b0, y_true_b2), \"Mis-match nos rótulos do teste\"\n",
        "y_true = y_true_b0\n",
        "\n",
        "# Ensemble por votação (maioria)\n",
        "votes = np.stack([y_pred_b0, y_pred_b1, y_pred_b2], axis=1)\n",
        "y_pred_ens = (np.sum(votes, axis=1) >= 2).astype(int)\n",
        "\n",
        "y_prob_ens = (y_prob_b0 + y_prob_b1 + y_prob_b2) / 3.0\n",
        "\n",
        "acc  = accuracy_score(y_true, y_pred_ens)\n",
        "prec = precision_score(y_true, y_pred_ens, zero_division=0)\n",
        "rec  = recall_score(y_true, y_pred_ens)\n",
        "f1   = f1_score(y_true, y_pred_ens)\n",
        "auc  = roc_auc_score(y_true, y_prob_ens)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred_ens).ravel()\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "print(\"\\nResultados — TESTE (ROI+PP+AD + Ensemble B0/B1/B2)\")\n",
        "print(f\"Acurácia:       {acc*100:6.2f}%\")\n",
        "print(f\"Especificidade: {specificity*100:6.2f}%\")\n",
        "print(f\"Sensibilidade:  {rec*100:6.2f}%\")\n",
        "print(f\"Precisão:       {prec*100:6.2f}%\")\n",
        "print(f\"F1-score:       {f1*100:6.2f}%\")\n",
        "print(f\"AUC-ROC:        {auc:8.5f}\")\n",
        "\n",
        "fpr, tpr, thr = roc_curve(y_true, y_prob_ens)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc:0.4f}\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Curva ROC — Teste (Ensemble)\")\n",
        "plt.legend(); plt.show()"
      ],
      "metadata": {
        "id": "4Guj77GNUEf2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}